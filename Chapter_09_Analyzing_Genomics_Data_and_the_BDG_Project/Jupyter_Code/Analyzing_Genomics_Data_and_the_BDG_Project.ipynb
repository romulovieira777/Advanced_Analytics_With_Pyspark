{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X0zdNqwnKgcv",
    "outputId": "dac2dc23-30d5-44ce-b065-8f810ad7cd08"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m317.0/317.0 MB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=25d75600c7636c8a457fc51a81a7a9a84eb62adf66300e4253522842f2881913\n",
      "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "Successfully installed pyspark-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"8g\").appName('chapter_9').getOrCreate()"
   ],
   "metadata": {
    "id": "F8Sl4L_ZKwtv"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!curl -O ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/phase3/data\\\n",
    "/HG00103/alignment/HG00103.mapped.ILLUMINA.bwa.GBR\\\n",
    ".low_coverage.20120522.bam"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTD68jjRKyvC",
    "outputId": "8461a722-77a2-4c2f-f0da-b0840ca7a02f"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "curl: (78) The file does not exist\n",
      "curl: (3) URL using bad/illegal format or missing URL\n",
      "curl: (6) Could not resolve host: .low_coverage.20120522.bam\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!mv HG00103.mapped.ILLUMINA.bwa.GBR\\\n",
    ".low_coverage.20120522.bam data/genomics"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ku1U9L1hLLf-",
    "outputId": "ac815e8f-a74f-4fa0-9636-a8221cde42fd"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mv: target 'data/genomics' is not a directory\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!adam-submit \\\n",
    "  --master yarn \\ 1\n",
    "  --deploy-mode client \\\n",
    "  --driver-memory 8G \\\n",
    "  --num-executors 6 \\\n",
    "  --executor-cores 4 \\\n",
    "  --executor-memory 12G \\\n",
    "  -- \\\n",
    "  transform \\ 2\n",
    "  data/genomics/HG00103.mapped.ILLUMINA.bwa.GBR\\\n",
    ".low_coverage.20120522.bam \\\n",
    "  data/genomics/HG00103"
   ],
   "metadata": {
    "id": "yqDdS0OiQRQE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!du -sh data/genomics/HG00103*bam\n",
    "16G  data/genomics/HG00103. [...] .bam\n",
    "\n",
    "!du -sh data/genomics/HG00103/\n",
    "13G  data/genomics/HG00103"
   ],
   "metadata": {
    "id": "XDUkCQIrRyHX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pyadam"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pyspark --conf spark.serializer=org.apache.spark.\n",
    "serializer.KryoSerializer --conf spark.kryo.registrator=\n",
    "org.bdgenomics.adam.serialization.ADAMKryoRegistrator\n",
    "--jars `find-adam-assembly.sh` --driver-class-path\n",
    "`find-adam-assembly.sh`"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from bdgenomics.adam.adamContext import ADAMContext\n",
    "\n",
    "ac = ADAMContext(spark)\n",
    "\n",
    "readsData = ac.loadAlignments(\"data/HG00103\")\n",
    "\n",
    "readsDataDF = readsData.toDF()\n",
    "readsDataDF.show(1, vertical=True)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "readsData.toDF().count()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "unique_chr = readsDataDF.select('referenceName').distinct().collect()\n",
    "unique_chr = [u.referenceName for u in unique_chr]\n",
    "\n",
    "unique_chr.sort()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import functions as fun\n",
    "\n",
    "cftr_reads = readsDataDF.where(\"referenceName == 7\").\\\n",
    "              where(fun.col(\"start\") <= 117149189).\\\n",
    "              where(fun.col(\"end\") > 117149189)\n",
    "\n",
    "cftr_reads.count()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!mkdir data/genomics/dnase"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!curl -O -L \"https://www.encodeproject.org/ \\\n",
    "              files/ENCFF001UVC/@@download/ENCFF001UVC.bed.gz\" | \\\n",
    "              gunzip > data/genomics/dnase/GM12878.DNase.narrowPeak 1\n",
    "!curl -O -L \"https://www.encodeproject.org/ \\\n",
    "              files/ENCFF001UWQ/@@download/ENCFF001UWQ.bed.gz\" | \\\n",
    "              gunzip > data/genomics/dnase/K562.DNase.narrowPeak\n",
    "!curl -O -L \"https://www.encodeproject.org/ \\\n",
    "              files/ENCFF001WEI/@@download/ENCFF001WEI.bed.gz\" | \\\n",
    "              gunzip > data/genomics/dnase/BJ.DNase.narrowPeak\n",
    "!curl -O -L \"https://www.encodeproject.org/ \\\n",
    "              files/ENCFF001UVQ/@@download/ENCFF001UVQ.bed.gz\" | \\\n",
    "              gunzip > data/genomics/dnase/HEK293.DNase.narrowPeak\n",
    "!curl -O -L \"https://www.encodeproject.org/ \\\n",
    "            files/ENCFF001SOM/@@download/ENCFF001SOM.bed.gz\" | \\\n",
    "            gunzip > data/genomics/dnase/H54.DNase.narrowPeak\n",
    "!curl -O -L \"https://www.encodeproject.org/ \\\n",
    "            files/ENCFF001UVU/@@download/ENCFF001UVU.bed.gz\" | \\\n",
    "            gunzip > data/genomics/dnase/HepG2.DNase.narrowPeak"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!mkdir data/genomics/chip-seq"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!curl -O -L \"https://www.encodeproject.org/ \\\n",
    "            files/ENCFF001VED/@@download/ENCFF001VED.bed.gz\" | \\\n",
    "            gunzip > data/genomics/chip-seq/GM12878.ChIP-seq.CTCF.narrowPeak\n",
    "!curl -O -L \"https://www.encodeproject.org/ \\\n",
    "            files/ENCFF001VMZ/@@download/ENCFF001VMZ.bed.gz\" | \\\n",
    "            gunzip > data/genomics/chip-seq/K562.ChIP-seq.CTCF.narrowPeak\n",
    "!!curl -O -L \"https://www.encodeproject.org/ \\\n",
    "            files/ENCFF001XMU/@@download/ENCFF001XMU.bed.gz\" | \\\n",
    "            gunzip > data/genomics/chip-seq/BJ.ChIP-seq.CTCF.narrowPeak\n",
    "!curl -O -L \"https://www.encodeproject.org/ \\\n",
    "            files/ENCFF001XQU/@@download/ENCFF001XQU.bed.gz\" | \\\n",
    "            gunzip > data/genomics/chip-seq/HEK293.ChIP-seq.CTCF.narrowPeak\n",
    "!curl -O -L \"https://www.encodeproject.org/ \\\n",
    "            files/ENCFF001USC/@@download/ENCFF001USC.bed.gz\" | \\\n",
    "            gunzip> data/genomics/chip-seq/H54.ChIP-seq.CTCF.narrowPeak\n",
    "!curl -O -L \"https://www.encodeproject.org/ \\\n",
    "            files/ENCFF001XRC/@@download/ENCFF001XRC.bed.gz\" | \\\n",
    "            gunzip> data/genomics/chip-seq/HepG2.ChIP-seq.CTCF.narrowPeak\n",
    "\n",
    "!curl -s -L \"http://ftp.ebi.ac.uk/pub/databases/gencode/\\\n",
    "            Gencode_human/release_18/gencode.v18.annotation.gtf.gz\" | \\\n",
    "            gunzip > data/genomics/gencode.v18.annotation.gtf"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cell_lines = [\"GM12878\", \"K562\", \"BJ\", \"HEK293\", \"H54\", \"HepG2\"]\n",
    "#for cell in cell_lines:\n",
    "## For each cell line…\n",
    "  ## …generate a suitable DataFrame\n",
    "## Concatenate the DataFrames and carry through into MLlib, for example"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ocal_prefix = \"data/genomics\"\n",
    "import pyspark.sql.functions as fun\n",
    "\n",
    "## UDF for finding closest transcription start site\n",
    "## naive; exercise for reader: make this faster\n",
    "def distance_to_closest(loci, query):\n",
    "  return min([abs(x - query) for x in loci])\n",
    "distance_to_closest_udf = fun.udf(distance_to_closest)\n",
    "\n",
    "\n",
    "## build in-memory structure for computing distance to TSS\n",
    "## we are essentially implementing a broadcast join here\n",
    "tss_data = ac.loadFeatures(\"data/genomics/gencode.v18.annotation.gtf\")\n",
    "tss_df = tss_data.toDF().filter(fun.col(\"featureType\") == 'transcript')\n",
    "b_tss_df = spark.sparkContext.broadcast(tss_df.groupBy('referenceName').\\\n",
    "                agg(fun.collect_list(\"start\").alias(\"start_sites\")))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "current_cell_line = cell_lines[0]\n",
    "\n",
    "dnase_path = f'data/genomics/dnase/{current_cell_line}.DNase.narrowPeak'\n",
    "dnase_data = ac.loadFeatures(dnase_path)\n",
    "dnase_data.toDF().columns"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chip_seq_path = f'data/genomics/chip-seq/ \\\n",
    "                  {current_cell_line}.ChIP-seq.CTCF.narrowPeak'\n",
    "chipseq_data = ac.loadFeatures(chipseq_path)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dnase_with_label = dnase_data.leftOuterShuffleRegionJoin(chipseq_data)\n",
    "dnase_with_label_df = dnase_with_label.toDF()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dnase_with_label_df = dnase_with_label_df.\\\n",
    "                        withColumn(\"label\", \\\n",
    "                                    ~fun.col(\"_2\").isNull())\n",
    "dnase_with_label_df.show(5)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## build final training DF\n",
    "training_df = dnase_with_label_df.withColumn(\n",
    "    \"contig\", fun.col(\"_1\").referenceName).withColumn(\n",
    "    \"start\", fun.col(\"_1\").start).withColumn(\n",
    "    \"end\", fun.col(\"_1\").end).withColumn(\n",
    "    \"tf\", fun.lit(\"CTCF\")).withColumn(\n",
    "    \"cell_line\", fun.lit(current_cell_line)).drop(\"_1\", \"_2\")\n",
    "\n",
    "training_df = training_df.join(b_tss_df,\n",
    "                               training_df.contig == b_tss_df.referenceName,\n",
    "                               \"inner\")\n",
    "\n",
    "training_df.withColumn(\"closest_tss\",\n",
    "                      fun.least(distance_to_closest_udf(fun.col(\"start_sites\"),\n",
    "                                                        fun.col(\"start\")),\n",
    "                          distance_to_closest_udf(fun.col(\"start_sites\"),\n",
    "                                                  fun.col(\"end\"))))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "preTrainingData = data_by_cellLine.union(...)\n",
    "preTrainingData.cache()\n",
    "\n",
    "preTrainingData.count()\n",
    "preTrainingData.filter(fun.col(\"label\") == true).count()"
   ]
  }
 ]
}
