{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T12:19:07.434274Z",
     "start_time": "2024-06-03T12:19:07.430444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ],
   "id": "87cc83afde451bda",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-03T12:19:48.951932Z",
     "start_time": "2024-06-03T12:19:09.173875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"8g\").\\\n",
    "                            config(\"spark.driver.maxResultSize\", \"0\").\\\n",
    "                            config(\"spark.kryoserializer.buffer.max\", \"2000M\").\\\n",
    "                            config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:4.0.2\").\\\n",
    "                            appName('chapter_6').\\\n",
    "                            getOrCreate()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Getting the Data",
   "id": "1370adb7deff8c23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!curl -s -L https://dumps.wikimedia.org/enwiki/latest/\\\n",
    "$ enwiki-latest-pages-articles-multistream.xml.bz2 \\\n",
    "$   | bzip2 -cd \\\n",
    "$   | hadoop fs -put - wikidump.xml"
   ],
   "id": "3ce15b216e86810f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! mv text wikidump",
   "id": "e836feb834bcb1cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! tree wikidump",
   "id": "8a19e653367736c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! head -n 5 wikidump/AA/wiki_00",
   "id": "642624524176c86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Spark NLP\n",
    "\n",
    "If you intend to use the PySpark shell, start the shell with the following command:\n",
    "\n",
    "```pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4```"
   ],
   "id": "6bbdce79db576ae2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install spark-nlp==3.2.3",
   "id": "dec40a4d9bfb5d8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4",
   "id": "b374ac8c9ad4ab91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()"
   ],
   "id": "e6893ce2bbd21b4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sparknlp.base import DocumentAssembler, Finisher\n",
    "from sparknlp.annotator import (Lemmatizer, Stemmer,\n",
    "                                Tokenizer, Normalizer,\n",
    "                                StopWordsCleaner)\n",
    "from sparknlp.pretrained import PretrainedPipeline"
   ],
   "id": "f7677eb9f12b64bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parsing the Data",
   "id": "5766491b8c82cdfa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_source = 'wikidump/*/*'",
   "id": "574f3206e9959b11"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "raw_data = spark.sparkContext.wholeTextFiles(data_source).toDF()\n",
    "raw_data.show(1, vertical=True)"
   ],
   "id": "66b31c120ae6ec5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import functions as fun\n",
    "\n",
    "df = raw_data.withColumn('content', fun.explode(fun.split(fun.col(\"_2\"),\n",
    "  \"</doc>\")))\n",
    "df = df.drop(fun.col('_2')).drop(fun.col('_1'))\n",
    "\n",
    "df.show(4, vertical=True)"
   ],
   "id": "7cd411afa41f7cc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.show(1, truncate=False, vertical=True)",
   "id": "e0364da9d2aae69a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = df.withColumn('title', fun.split(fun.col('content'), '\\n').getItem(2)) \\\n",
    "       .withColumn('content', fun.split(fun.col('content'), '\\n').getItem(4))\n",
    "df.show(4, vertical=True)"
   ],
   "id": "4ab74ec7216cb33a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preparing the Data Using Spark NLP",
   "id": "6d03e15e41cfa0b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"content\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink\")\n",
    "\n",
    "document_assembler.transform(df).select('document').limit(1).collect()"
   ],
   "id": "b2f180710bac1c42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split sentence to tokens(array)\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")"
   ],
   "id": "aff22dc50ee9d981"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clean unwanted characters and garbage\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\") \\\n",
    "    .setLowercase(True)"
   ],
   "id": "e2cf1ffbaa6bcf3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove stopwords\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"normalized\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)"
   ],
   "id": "acde9df0a0706d6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Stem the words to bring them to the root form.\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"stem\")"
   ],
   "id": "f8e74750f85df80d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)"
   ],
   "id": "37cf94521ecd604b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[document_assembler,\n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            stopwords_cleaner,\n",
    "            stemmer,\n",
    "            finisher])"
   ],
   "id": "cb21ef0491f756b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nlp_model = nlp_pipeline.fit(df)\n",
    "\n",
    "processed_df  = nlp_model.transform(df)\n",
    "\n",
    "processed_df.printSchema()"
   ],
   "id": "a5d07a2ea00cbc83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokens_df = processed_df.select('title', 'tokens')\n",
    "tokens_df.show(2, vertical=True)"
   ],
   "id": "473524effeeeaa49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Computing the TF-IDFa",
   "id": "7ae1591d7c33e245"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import math\n",
    "\n",
    "def term_doc_weight(term_frequency_in_doc, total_terms_in_doc,\n",
    "                    term_freq_in_corpus, total_docs):\n",
    "    tf = term_frequency_in_doc / total_terms_in_doc\n",
    "    doc_freq = total_docs / term_freq_in_corpus\n",
    "    idf = math.log(doc_freq)\n",
    "    tf * idf"
   ],
   "id": "6228094fae93e1aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\")\n",
    "\n",
    "# train the model\n",
    "cv_model = cv.fit(tokens_df)\n",
    "\n",
    "# transform the data. Output column name will be raw_features.\n",
    "vectorized_tokens = cv_model.transform(tokens_df)"
   ],
   "id": "6e3b9ff1df434c5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "idf_model = idf.fit(vectorized_tokens)\n",
    "\n",
    "vectorized_df = idf_model.transform(vectorized_tokens)"
   ],
   "id": "d82605b1f602fadc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vectorized_df = vectorized_df.drop(fun.col('raw_features'))\n",
    "\n",
    "vectorized_df.show(6)"
   ],
   "id": "406cfffbe9e22337"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Creating Our LDA Model",
   "id": "f763ca27dab5cf96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "num_topics = 5\n",
    "max_iter = 50\n",
    "\n",
    "lda = LDA(k=num_topics, maxIter=max_iter)\n",
    "model = lda.fit(vectorized_df)\n",
    "\n",
    "lp = model.logPerplexity(vectorized_df)\n",
    "\n",
    "print(\"The upper bound on perplexity: \" + str(lp))"
   ],
   "id": "7183da09391cb9b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vocab = cv_model.vocabulary\n",
    "\n",
    "raw_topics = model.describeTopics().collect()\n",
    "\n",
    "topic_inds = [ind.termIndices for ind in raw_topics]\n",
    "\n",
    "topics = []\n",
    "for topic in topic_inds:\n",
    "    _topic = []\n",
    "    for ind in topic:\n",
    "        _topic.append(vocab[ind])\n",
    "    topics.append(_topic)"
   ],
   "id": "d4884c98c93e3ace"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i, topic in enumerate(topics, start=1):\n",
    "    print(f\"topic {i}: {topic}\")"
   ],
   "id": "4a63cc00bfc7ec2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lda_df = model.transform(vectorized_df)\n",
    "lda_df.select(fun.col('title'), fun.col('topicDistribution')).\\\n",
    "                show(2, vertical=True, truncate=False)"
   ],
   "id": "c609d440c5af206c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "max_index = fun.udf(lambda x: x.tolist().index(max(x)) + 1, IntegerType())\n",
    "lda_df = lda_df.withColumn('topic_index',\n",
    "                        max_index(fun.col('topicDistribution')))"
   ],
   "id": "f89f7188f30b7c96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "lda_df.select('title', 'topic_index').show(10, truncate=False)",
   "id": "7446f560e1d25b87"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
